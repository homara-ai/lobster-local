name: Comprehensive CI Pipeline

on:
  push:
    branches: [ main, develop, tests ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC to catch dependency issues
    - cron: '0 2 * * *'

env:
  # Environment variables for testing
  PYTHONPATH: ${{ github.workspace }}
  PYTEST_ADDOPTS: "--color=yes --tb=short --strict-markers"
  
jobs:
  # =====================================================================
  # Code Quality and Linting
  # =====================================================================
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pre-commit bandit safety
    
    - name: Cache pre-commit hooks
      uses: actions/cache@v3
      with:
        path: ~/.cache/pre-commit
        key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
    
    - name: Run pre-commit hooks
      run: pre-commit run --all-files --show-diff-on-failure
    
    - name: Security check with bandit
      run: bandit -r lobster/ -f json -o bandit-report.json || true
    
    - name: Safety check for dependencies
      run: safety check --json --output safety-report.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  # =====================================================================
  # Unit Tests (Fast feedback)
  # =====================================================================
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.11", "3.12"]
        exclude:
          # Exclude Windows Python 3.11 to reduce CI load
          - os: windows-latest
            python-version: "3.11"
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Create test environment
      run: |
        mkdir -p tests/.test_cache
        echo "LOBSTER_TEST_MODE=true" >> $GITHUB_ENV
        echo "LOBSTER_WORKSPACE_PATH=tests/.test_workspace" >> $GITHUB_ENV
    
    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ \
          --maxfail=5 \
          --durations=10 \
          --cov=lobster \
          --cov-branch \
          --cov-report=xml \
          --cov-report=html \
          --junit-xml=unit-test-results.xml \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: unit-tests-${{ matrix.python-version }}
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          unit-test-results.xml
          htmlcov/
        retention-days: 30

  # =====================================================================
  # Integration Tests 
  # =====================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [code-quality, unit-tests]
    
    services:
      # Mock external services for integration testing
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libhdf5-dev \
          libxml2-dev \
          libxslt1-dev \
          zlib1g-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Set up test environment
      run: |
        mkdir -p tests/.test_cache tests/.test_workspace
        echo "LOBSTER_TEST_MODE=true" >> $GITHUB_ENV
        echo "LOBSTER_WORKSPACE_PATH=tests/.test_workspace" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379" >> $GITHUB_ENV
    
    - name: Generate test data
      run: |
        python -c "
        import sys
        sys.path.insert(0, 'tests')
        from mock_data.factories import SingleCellDataFactory, BulkRNASeqDataFactory
        from mock_data.base import SMALL_DATASET_CONFIG
        
        # Pre-generate test data for faster integration tests
        sc_data = SingleCellDataFactory(config=SMALL_DATASET_CONFIG)
        bulk_data = BulkRNASeqDataFactory(config=SMALL_DATASET_CONFIG)
        
        # Save to cache
        import os
        os.makedirs('tests/.test_cache', exist_ok=True)
        sc_data.write_h5ad('tests/.test_cache/test_single_cell.h5ad')
        print('âœ“ Test data generated successfully')
        "
    
    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ \
          --maxfail=3 \
          --durations=10 \
          --cov=lobster \
          --cov-append \
          --cov-report=xml \
          --junit-xml=integration-test-results.xml \
          --timeout=300 \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration-tests
        name: integration-tests
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          integration-test-results.xml
          tests/.test_workspace/
        retention-days: 30

  # =====================================================================
  # System Tests (End-to-End)
  # =====================================================================
  system-tests:
    name: System Tests (E2E)
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [integration-tests]
    if: github.event_name != 'schedule' # Skip on scheduled runs to save resources
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Set up comprehensive test environment
      run: |
        mkdir -p tests/.test_cache tests/.test_workspace tests/system/.temp
        echo "LOBSTER_TEST_MODE=true" >> $GITHUB_ENV
        echo "LOBSTER_WORKSPACE_PATH=tests/.test_workspace" >> $GITHUB_ENV
        echo "LOBSTER_LOG_LEVEL=INFO" >> $GITHUB_ENV
    
    - name: Run system tests with retry
      uses: nick-invision/retry@v2
      with:
        timeout_minutes: 50
        max_attempts: 2
        retry_on: error
        command: |
          python -m pytest tests/system/ \
            --maxfail=2 \
            --durations=20 \
            --cov=lobster \
            --cov-append \
            --cov-report=xml \
            --junit-xml=system-test-results.xml \
            --timeout=600 \
            --capture=no \
            -v
    
    - name: Run enhanced integration test runner
      run: |
        python tests/run_integration_tests.py \
          --input tests/test_cases.json \
          --output tests/system-integration-results.json \
          --categories basic,advanced \
          --log-level INFO \
          --performance-monitoring
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: system-tests
        name: system-tests
    
    - name: Upload comprehensive test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: system-test-results
        path: |
          system-test-results.xml
          tests/system-integration-results.json
          tests/.test_workspace/
        retention-days: 30

  # =====================================================================
  # Performance Tests
  # =====================================================================
  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 90
    needs: [unit-tests]
    if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install memory-profiler psutil
    
    - name: Set up performance test environment
      run: |
        mkdir -p tests/.test_cache tests/.perf_results
        echo "LOBSTER_TEST_MODE=true" >> $GITHUB_ENV
        echo "LOBSTER_WORKSPACE_PATH=tests/.test_workspace" >> $GITHUB_ENV
        echo "LOBSTER_PERFORMANCE_MODE=true" >> $GITHUB_ENV
    
    - name: Run performance tests
      run: |
        python -m pytest tests/performance/ \
          --maxfail=1 \
          --durations=0 \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --junit-xml=performance-test-results.xml \
          --timeout=1200 \
          -v
    
    - name: Generate performance report
      run: |
        python -c "
        import json
        import sys
        
        try:
            with open('benchmark-results.json', 'r') as f:
                data = json.load(f)
            
            benchmarks = data.get('benchmarks', [])
            if benchmarks:
                print('## Performance Benchmark Results')
                print('| Test | Mean Time | Min Time | Max Time | Ops/sec |')
                print('|------|-----------|----------|----------|---------|')
                
                for bench in benchmarks:
                    name = bench['name']
                    stats = bench['stats']
                    mean = f\"{stats['mean']:.4f}s\"
                    min_time = f\"{stats['min']:.4f}s\"
                    max_time = f\"{stats['max']:.4f}s\"
                    ops_per_sec = f\"{1/stats['mean']:.2f}\" if stats['mean'] > 0 else 'N/A'
                    print(f'| {name} | {mean} | {min_time} | {max_time} | {ops_per_sec} |')
            else:
                print('No benchmark results found')
        except Exception as e:
            print(f'Error processing benchmark results: {e}')
        " | tee performance-report.md
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: |
          performance-test-results.xml
          benchmark-results.json
          performance-report.md
        retention-days: 60

  # =====================================================================
  # Security and Dependency Scanning
  # =====================================================================
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  # =====================================================================
  # Build and Package Testing
  # =====================================================================
  build-test:
    name: Build and Package Test
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine wheel
    
    - name: Build package
      run: python -m build
    
    - name: Check package
      run: python -m twine check dist/*
    
    - name: Test installation from wheel
      run: |
        pip install dist/*.whl
        python -c "import lobster; print(f'Lobster version: {lobster.__version__}')"
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: build-artifacts
        path: dist/
        retention-days: 30

  # =====================================================================
  # Documentation Build Test
  # =====================================================================
  docs-build:
    name: Documentation Build
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install documentation dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install sphinx sphinx-rtd-theme myst-parser
    
    - name: Build documentation
      run: |
        if [ -d "docs" ]; then
          cd docs
          make html
        else
          echo "No docs directory found, skipping documentation build"
        fi

  # =====================================================================
  # Test Results Summary
  # =====================================================================
  test-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, system-tests, performance-tests, build-test]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate test summary
      run: |
        echo "# Test Results Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "## Job Status" >> test-summary.md
        echo "- Code Quality: ${{ needs.code-quality.result }}" >> test-summary.md
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> test-summary.md
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> test-summary.md
        echo "- System Tests: ${{ needs.system-tests.result }}" >> test-summary.md
        echo "- Performance Tests: ${{ needs.performance-tests.result }}" >> test-summary.md
        echo "- Build Test: ${{ needs.build-test.result }}" >> test-summary.md
        echo "" >> test-summary.md
        
        # Count test files and results
        find . -name "*test-results.xml" -exec echo "Found test results: {}" \;
        
        echo "## Coverage Information" >> test-summary.md
        if [ -f "*/coverage.xml" ]; then
          echo "Coverage reports generated successfully" >> test-summary.md
        else
          echo "No coverage reports found" >> test-summary.md
        fi
    
    - name: Upload test summary
      uses: actions/upload-artifact@v3
      with:
        name: test-summary
        path: test-summary.md
        retention-days: 90

  # =====================================================================
  # Deployment (on successful tests)
  # =====================================================================
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Deploy to staging environment
      run: |
        echo "Deploying to staging environment..."
        # Add actual deployment steps here
        echo "Deployment completed successfully"

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Deploy to production environment
      run: |
        echo "Deploying to production environment..."
        # Add actual deployment steps here
        echo "Production deployment completed successfully"