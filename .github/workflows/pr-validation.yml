name: Pull Request Validation

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [main, develop]
  pull_request_review:
    types: [submitted]

env:
  PYTHONPATH: ${{ github.workspace }}
  PYTEST_ADDOPTS: "--color=yes --tb=short --strict-markers"

jobs:
  # =====================================================================
  # PR Metadata and Change Analysis
  # =====================================================================
  pr-analysis:
    name: PR Analysis and Labeling
    runs-on: ubuntu-latest
    if: github.event.pull_request.draft == false
    outputs:
      has_core_changes: ${{ steps.changes.outputs.core }}
      has_test_changes: ${{ steps.changes.outputs.tests }}
      has_agent_changes: ${{ steps.changes.outputs.agents }}
      has_docs_changes: ${{ steps.changes.outputs.docs }}
      complexity_score: ${{ steps.complexity.outputs.score }}
      
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        ref: ${{ github.event.pull_request.head.sha }}
    
    - name: Analyze changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          core:
            - 'lobster/core/**'
            - 'lobster/agents/**'
          tests:
            - 'tests/**'
            - 'pyproject.toml'
          agents:
            - 'lobster/agents/**'
          docs:
            - 'docs/**'
            - '*.md'
            - 'CHANGELOG.md'
          config:
            - 'pyproject.toml'
            - 'setup.py'
            - '.github/**'
    
    - name: Calculate complexity score
      id: complexity
      run: |
        # Calculate PR complexity based on various factors
        files_changed=$(git diff --name-only origin/${{ github.event.pull_request.base.ref }}..HEAD | wc -l)
        lines_added=$(git diff --numstat origin/${{ github.event.pull_request.base.ref }}..HEAD | awk '{sum+=$1} END {print sum}')
        lines_removed=$(git diff --numstat origin/${{ github.event.pull_request.base.ref }}..HEAD | awk '{sum+=$2} END {print sum}')
        
        # Simple complexity scoring
        complexity=0
        if [ $files_changed -gt 20 ]; then complexity=$((complexity + 3)); fi
        if [ $lines_added -gt 500 ]; then complexity=$((complexity + 2)); fi
        if [ $lines_removed -gt 300 ]; then complexity=$((complexity + 2)); fi
        
        # Check for core changes
        if [ "${{ steps.changes.outputs.core }}" == "true" ]; then
          complexity=$((complexity + 2))
        fi
        
        echo "Files changed: $files_changed"
        echo "Lines added: $lines_added"
        echo "Lines removed: $lines_removed"
        echo "Complexity score: $complexity"
        echo "score=$complexity" >> $GITHUB_OUTPUT
    
    - name: Add labels based on changes
      uses: actions/labeler@v4
      with:
        configuration-path: .github/labeler.yml
        repo-token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Comment on PR with analysis
      uses: actions/github-script@v6
      with:
        script: |
          const complexity = ${{ steps.complexity.outputs.score }};
          const filesChanged = await github.rest.repos.compareCommits({
            owner: context.repo.owner,
            repo: context.repo.repo,
            base: '${{ github.event.pull_request.base.sha }}',
            head: '${{ github.event.pull_request.head.sha }}'
          });
          
          let complexityLevel = 'Low';
          if (complexity >= 7) complexityLevel = 'High';
          else if (complexity >= 4) complexityLevel = 'Medium';
          
          const comment = `
          ## 🔍 PR Analysis
          
          **Complexity Level:** ${complexityLevel} (Score: ${complexity})
          **Files Changed:** ${filesChanged.data.files.length}
          **Changes Detected:**
          - Core changes: ${{ steps.changes.outputs.core == 'true' ? '✅' : '❌' }}
          - Test changes: ${{ steps.changes.outputs.tests == 'true' ? '✅' : '❌' }}
          - Agent changes: ${{ steps.changes.outputs.agents == 'true' ? '✅' : '❌' }}
          - Documentation changes: ${{ steps.changes.outputs.docs == 'true' ? '✅' : '❌' }}
          
          ${complexity >= 7 ? '⚠️ **High complexity PR** - Consider breaking into smaller PRs if possible.' : ''}
          ${complexity >= 4 ? '📋 **Medium complexity PR** - Ensure thorough testing and review.' : ''}
          `;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # =====================================================================
  # Fast Feedback Tests (Unit + Linting)
  # =====================================================================
  fast-feedback:
    name: Fast Feedback (Linting + Unit Tests)
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run pre-commit hooks
      run: |
        pre-commit install
        pre-commit run --all-files --show-diff-on-failure
    
    - name: Run unit tests with coverage
      run: |
        python -m pytest tests/unit/ \
          --maxfail=10 \
          --cov=lobster \
          --cov-branch \
          --cov-report=xml \
          --junit-xml=fast-test-results.xml \
          --durations=10 \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: pr-fast-feedback
        name: pr-unit-tests

  # =====================================================================
  # Security and Safety Checks  
  # =====================================================================
  security-checks:
    name: Security and Safety Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety pip-audit semgrep
        pip install -e ".[dev]"
    
    - name: Run Bandit security linter
      run: |
        bandit -r lobster/ -f json -o bandit-results.json
        bandit -r lobster/ -f txt
    
    - name: Run Safety dependency scanner
      run: |
        safety check --json --output safety-results.json
        safety check
    
    - name: Run pip-audit for vulnerabilities
      run: pip-audit --format=json --output=pip-audit-results.json
    
    - name: Run Semgrep security scanner
      run: |
        semgrep --config=auto --json --output=semgrep-results.json lobster/
        semgrep --config=auto lobster/
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: pr-security-scan-results
        path: |
          bandit-results.json
          safety-results.json
          pip-audit-results.json
          semgrep-results.json
        retention-days: 30

  # =====================================================================
  # Change-Specific Tests
  # =====================================================================
  change-specific-tests:
    name: Change-Specific Tests
    runs-on: ubuntu-latest
    needs: [pr-analysis, fast-feedback]
    timeout-minutes: 30
    if: needs.pr-analysis.outputs.has_core_changes == 'true' || needs.pr-analysis.outputs.has_agent_changes == 'true'
    
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run integration tests for core changes
      if: needs.pr-analysis.outputs.has_core_changes == 'true'
      run: |
        python -m pytest tests/integration/ \
          --maxfail=5 \
          --cov=lobster \
          --cov-append \
          --cov-report=xml \
          --junit-xml=integration-test-results.xml \
          --timeout=300 \
          -v
    
    - name: Run agent-specific tests
      if: needs.pr-analysis.outputs.has_agent_changes == 'true'
      run: |
        python -m pytest tests/unit/agents/ tests/integration/ \
          -k "agent" \
          --maxfail=5 \
          --cov=lobster.agents \
          --cov-append \
          --cov-report=xml \
          --junit-xml=agent-test-results.xml \
          --timeout=300 \
          -v
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: change-specific-test-results
        path: |
          integration-test-results.xml
          agent-test-results.xml
        retention-days: 30

  # =====================================================================
  # Cross-Platform Compatibility
  # =====================================================================
  cross-platform-tests:
    name: Cross-Platform Tests (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    needs: [pr-analysis]
    timeout-minutes: 25
    if: needs.pr-analysis.outputs.complexity_score >= 4
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ["3.11", "3.12"]
        exclude:
          # Run full matrix only for high complexity PRs
          - os: macos-latest
            python-version: "3.11"
    
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run core test suite
      run: |
        python -m pytest tests/unit/ tests/integration/ \
          --maxfail=3 \
          --durations=10 \
          --junit-xml=cross-platform-results-${{ matrix.os }}-${{ matrix.python-version }}.xml \
          -v
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cross-platform-test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: cross-platform-results-${{ matrix.os }}-${{ matrix.python-version }}.xml
        retention-days: 30

  # =====================================================================
  # Performance Impact Assessment
  # =====================================================================
  performance-impact:
    name: Performance Impact Assessment
    runs-on: ubuntu-latest
    needs: [pr-analysis]
    timeout-minutes: 30
    if: needs.pr-analysis.outputs.has_core_changes == 'true'
    
    steps:
    - name: Checkout base branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.base.ref }}
        path: base
    
    - name: Checkout PR branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        path: pr
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: 'pip'
    
    - name: Run performance benchmarks on base
      run: |
        cd base
        pip install -e ".[dev]"
        python -m pytest tests/performance/ \
          --benchmark-only \
          --benchmark-json=../base-benchmark.json \
          --maxfail=1 \
          || echo "Base benchmark failed"
    
    - name: Run performance benchmarks on PR
      run: |
        cd pr
        pip install -e ".[dev]" --force-reinstall
        python -m pytest tests/performance/ \
          --benchmark-only \
          --benchmark-json=../pr-benchmark.json \
          --maxfail=1 \
          || echo "PR benchmark failed"
    
    - name: Compare performance results
      run: |
        python -c "
        import json
        import sys
        
        def load_benchmark(file_path):
            try:
                with open(file_path, 'r') as f:
                    return json.load(f)
            except:
                return None
        
        base_data = load_benchmark('base-benchmark.json')
        pr_data = load_benchmark('pr-benchmark.json')
        
        if not base_data or not pr_data:
            print('⚠️  Could not compare benchmarks - missing data')
            sys.exit(0)
        
        print('## Performance Comparison')
        print('| Test | Base Time | PR Time | Change |')
        print('|------|-----------|---------|--------|')
        
        base_benchmarks = {b['name']: b['stats']['mean'] for b in base_data.get('benchmarks', [])}
        pr_benchmarks = {b['name']: b['stats']['mean'] for b in pr_data.get('benchmarks', [])}
        
        significant_regressions = []
        
        for name in base_benchmarks:
            if name in pr_benchmarks:
                base_time = base_benchmarks[name]
                pr_time = pr_benchmarks[name]
                change_pct = ((pr_time - base_time) / base_time) * 100
                
                change_str = f'{change_pct:+.1f}%'
                if abs(change_pct) > 10:  # Significant change
                    change_str += ' ⚠️'
                    if change_pct > 0:  # Regression
                        significant_regressions.append((name, change_pct))
                
                print(f'| {name} | {base_time:.4f}s | {pr_time:.4f}s | {change_str} |')
        
        if significant_regressions:
            print()
            print('### ⚠️  Significant Performance Regressions Detected')
            for name, pct in significant_regressions:
                if pct > 20:
                    print(f'- **{name}**: {pct:.1f}% slower (CRITICAL)')
                else:
                    print(f'- **{name}**: {pct:.1f}% slower')
        else:
            print()
            print('✅ No significant performance regressions detected')
        " > performance-comparison.md
        
        cat performance-comparison.md
    
    - name: Upload performance comparison
      uses: actions/upload-artifact@v3
      with:
        name: performance-comparison
        path: |
          performance-comparison.md
          base-benchmark.json
          pr-benchmark.json
        retention-days: 30
    
    - name: Comment performance results
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const comparison = fs.readFileSync('performance-comparison.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '## 🚀 Performance Impact Analysis\n\n' + comparison
            });
          } catch (error) {
            console.log('Could not post performance comparison:', error);
          }

  # =====================================================================
  # Documentation and Examples Validation
  # =====================================================================
  docs-validation:
    name: Documentation Validation
    runs-on: ubuntu-latest
    needs: [pr-analysis]
    if: needs.pr-analysis.outputs.has_docs_changes == 'true' || needs.pr-analysis.outputs.has_core_changes == 'true'
    
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install sphinx sphinx-rtd-theme myst-parser
    
    - name: Validate documentation links
      run: |
        # Check for broken internal links in markdown files
        find . -name "*.md" -exec grep -l "http" {} \; | while read file; do
          echo "Checking links in $file"
          # Add link validation logic here
        done
    
    - name: Build documentation
      run: |
        if [ -d "docs" ]; then
          cd docs
          make html
        else
          echo "No docs directory found"
        fi
    
    - name: Validate code examples in documentation
      run: |
        # Extract and validate Python code blocks from documentation
        python -c "
        import re
        import ast
        import glob
        
        def validate_code_block(code):
            try:
                ast.parse(code)
                return True, None
            except SyntaxError as e:
                return False, str(e)
        
        markdown_files = glob.glob('**/*.md', recursive=True)
        
        for file_path in markdown_files:
            with open(file_path, 'r') as f:
                content = f.read()
            
            # Find Python code blocks
            python_blocks = re.findall(r'\`\`\`python\n(.*?)\n\`\`\`', content, re.DOTALL)
            
            for i, block in enumerate(python_blocks):
                valid, error = validate_code_block(block)
                if not valid:
                    print(f'❌ Invalid Python code in {file_path}, block {i+1}: {error}')
                else:
                    print(f'✅ Valid Python code in {file_path}, block {i+1}')
        "

  # =====================================================================
  # PR Approval Requirements Check
  # =====================================================================
  approval-check:
    name: PR Approval Requirements
    runs-on: ubuntu-latest
    needs: [pr-analysis, fast-feedback, security-checks]
    if: always()
    
    steps:
    - name: Check approval requirements
      uses: actions/github-script@v6
      with:
        script: |
          const complexity = ${{ needs.pr-analysis.outputs.complexity_score }};
          const { data: reviews } = await github.rest.pulls.listReviews({
            owner: context.repo.owner,
            repo: context.repo.repo,
            pull_number: context.issue.number
          });
          
          const approvals = reviews.filter(review => review.state === 'APPROVED').length;
          const requestedChanges = reviews.filter(review => review.state === 'CHANGES_REQUESTED').length;
          
          let requiredApprovals = 1;
          if (complexity >= 7) requiredApprovals = 2;
          if (complexity >= 10) requiredApprovals = 3;
          
          const canMerge = approvals >= requiredApprovals && requestedChanges === 0;
          
          const statusComment = `
          ## 🔍 PR Approval Status
          
          **Current Approvals:** ${approvals}
          **Required Approvals:** ${requiredApprovals} (based on complexity: ${complexity})
          **Changes Requested:** ${requestedChanges}
          
          **Status:** ${canMerge ? '✅ Ready to merge' : '⏳ Awaiting approval'}
          
          ${complexity >= 7 ? '⚠️ High complexity PR requires additional review.' : ''}
          `;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: statusComment
          });

  # =====================================================================
  # Final PR Status Summary
  # =====================================================================
  pr-status-summary:
    name: PR Validation Summary
    runs-on: ubuntu-latest
    needs: [
      pr-analysis, 
      fast-feedback, 
      security-checks, 
      change-specific-tests, 
      cross-platform-tests,
      performance-impact,
      docs-validation
    ]
    if: always()
    
    steps:
    - name: Generate PR validation summary
      uses: actions/github-script@v6
      with:
        script: |
          const jobs = {
            'PR Analysis': '${{ needs.pr-analysis.result }}',
            'Fast Feedback': '${{ needs.fast-feedback.result }}', 
            'Security Checks': '${{ needs.security-checks.result }}',
            'Change-Specific Tests': '${{ needs.change-specific-tests.result }}',
            'Cross-Platform Tests': '${{ needs.cross-platform-tests.result }}',
            'Performance Impact': '${{ needs.performance-impact.result }}',
            'Documentation Validation': '${{ needs.docs-validation.result }}'
          };
          
          const getIcon = (status) => {
            switch(status) {
              case 'success': return '✅';
              case 'failure': return '❌';
              case 'cancelled': return '⏹️';
              case 'skipped': return '⏭️';
              default: return '⏳';
            }
          };
          
          let summary = '## 🎯 PR Validation Summary\n\n';
          
          for (const [jobName, status] of Object.entries(jobs)) {
            if (status && status !== 'skipped') {
              summary += `${getIcon(status)} **${jobName}**: ${status}\n`;
            }
          }
          
          const allPassed = Object.values(jobs).every(status => 
            status === 'success' || status === 'skipped' || !status
          );
          
          summary += `\n**Overall Status:** ${allPassed ? '✅ All validations passed' : '❌ Some validations failed'}`;
          
          if (!allPassed) {
            summary += '\n\n⚠️ Please address the failing checks before merging.';
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });